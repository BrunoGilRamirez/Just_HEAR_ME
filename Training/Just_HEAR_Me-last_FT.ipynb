{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68cb3d53",
   "metadata": {},
   "source": [
    "# Training S칩lo Esc칰chame: Spanish Emotional Accompaniment Chatbot 游눫游뱄\n",
    "\n",
    "This notebook presents the final iteration of the training process, which was part of the experimental phase reported in the article [S칩lo Esc칰chame: Spanish Emotional Accompaniment Chatbot](https://arxiv.org/).\n",
    "\n",
    "This model uses [Llama2-7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) as the base model, fine-tuning it for the task of emotional accompaniment. The training utilizes the [HEAR](https://huggingface.co/datasets/BrunoGR/HEAR-Hispanic_Emotional_Accompaniment_Responses) (Hispanic Emotional Accompaniment Responses) dataset, proposed in the same article as this model. The objective is to tailor the model's responses to provide emotional support in various situations.\n",
    "\n",
    "HEAR is based on another dataset, [HRECPW](https://huggingface.co/datasets/BrunoGR/HRECPW-Hispanic_Responses_for_Emotional_Classification_based_on_Plutchik_Wheel), which compiles elements from datasets focused on emotion tagging and classification. It unites these elements into a dataset with 11 emotion categories: Affection, Joy, Admiration, Anger, Sadness, Optimism, Hatred, Surprise, Fear, Calm, and Disgust.\n",
    "\n",
    "The elements comprising HEAR were sampled randomly from HRECPW, and the responses for these elements were generated using OpenAI's GPT-3.5-Turbo API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f30b8",
   "metadata": {},
   "source": [
    "## Let췂s start to code\n",
    "1. It is needed to install some dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a2b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc72f31",
   "metadata": {},
   "source": [
    "2. import the installed dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc9050-de55-4e76-8eb3-796b0d34d5e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T00:16:07.508572Z",
     "iopub.status.busy": "2023-12-04T00:16:07.508352Z",
     "iopub.status.idle": "2023-12-04T00:16:13.873098Z",
     "shell.execute_reply": "2023-12-04T00:16:13.872416Z",
     "shell.execute_reply.started": "2023-12-04T00:16:07.508547Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments, \n",
    "    EarlyStoppingCallback, \n",
    "    LlamaTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    ")\n",
    "from peft import ( \n",
    "    LoraConfig, \n",
    "    PeftModel,\n",
    "    get_peft_model\n",
    ")\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from huggingface_hub import login  \n",
    "import pandas as pd\n",
    "import wandb, datasets, os, load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d960465b",
   "metadata": {},
   "source": [
    "3. load the environmental variables as access tokens for wandb and huggingface_hub to track the training progress and plot the metrics, and access to models and datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a1a5c-d2fd-4cf0-8dc2-f73cf415a66c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T00:16:13.874539Z",
     "iopub.status.busy": "2023-12-04T00:16:13.874158Z",
     "iopub.status.idle": "2023-12-04T00:16:13.925369Z",
     "shell.execute_reply": "2023-12-04T00:16:13.924783Z",
     "shell.execute_reply.started": "2023-12-04T00:16:13.874520Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dotenv.load_dotenv()\n",
    "access_token = os.getenv('acesstoken')\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Model for training\n",
    "1. Load the model: In this iteration a A100 of 80GB of VRAM was used for ~18hrs so we dispensed to quantized the 7 billion parameter model. The goal is to get all model parameters with the most precision that 32 bit can give. \n",
    "2. tie_weights(): To maximize the efficiency of memory when the model is loaded, tie the weights between the input embedding layer and the output embedding layer with [tie_weights()](https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.tie_weights). \n",
    "3. Load the Tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106b9792-6725-4979-b625-57ba3e3f7d81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T00:16:13.927031Z",
     "iopub.status.busy": "2023-12-04T00:16:13.926855Z",
     "iopub.status.idle": "2023-12-04T00:20:31.464171Z",
     "shell.execute_reply": "2023-12-04T00:20:31.463635Z",
     "shell.execute_reply.started": "2023-12-04T00:16:13.927014Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters=\"7b-chat\"\n",
    "BASE_MODEL = f\"meta-llama/Llama-2-{parameters}-hf\"\n",
    "# if there is a pretrained model, load it the model is Models_of_Llama/Llama_base\n",
    "myModel= \"BrunoGR/EmotionalBot_LLaMA2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    #torch_dtype=torch.float16,\n",
    "    #quantization_config=bnb_config,\n",
    "    #load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.tie_weights()\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "tokenizer =   LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c789a",
   "metadata": {},
   "source": [
    "4.  Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95d992-b93b-4c78-9a9d-42dbda08a9bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T00:20:31.465608Z",
     "iopub.status.busy": "2023-12-04T00:20:31.464858Z",
     "iopub.status.idle": "2023-12-04T00:20:34.706089Z",
     "shell.execute_reply": "2023-12-04T00:20:34.705567Z",
     "shell.execute_reply.started": "2023-12-04T00:20:31.465585Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data= datasets.load_dataset(\"BrunoGR/HEAR-Hispanic_Emotional_Accompaniment_Responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4438fc81",
   "metadata": {},
   "source": [
    "5. Set the LoRA Parameters, to acquire a bigger adapter to modify the model weights during the training process, the 'r' Value must be increased, and to get a bigger scale of modification to model weights increase the lora_alpha is necessary. Because Llama2 is not completely multilingual, significant changes must be made to acquire better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "240e8a1d-82bd-49f7-a07c-e7a176ac5c24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T00:20:34.707371Z",
     "iopub.status.busy": "2023-12-04T00:20:34.707195Z",
     "iopub.status.idle": "2023-12-04T00:20:34.710631Z",
     "shell.execute_reply": "2023-12-04T00:20:34.710168Z",
     "shell.execute_reply.started": "2023-12-04T00:20:34.707355Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LoRA_TARGET_MODULES = [ \n",
    "    \"q_proj\", \n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "LoRA_DROPOUT= 0.1\n",
    "config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    target_modules=LoRA_TARGET_MODULES,\n",
    "    lora_dropout=LoRA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10dd691",
   "metadata": {},
   "source": [
    "(optional) If you want to see how many parameters are going to be trained disable the '''. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d91f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pftmdl = get_peft_model(model, config)\n",
    "pftmdl.print_trainable_parameters()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796cf1db",
   "metadata": {},
   "source": [
    "### 6. The training parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdfbce7",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "The model training is configured with a total batch size of $15$ and a micro batch size of $5$ to optimize GPU memory and training stability, accumulating gradients over $3$ steps to simulate a larger batch size. A warm-up of $300$ steps is used to stabilize the learning rate, starting at $5e-5$, with AdamW hyperparameters recommended in the LLaMA paper, including adam_beta1 of $0.9$, adam_beta2 of $0.95$, and adam_epsilon of $2e-8$. Weight decay of $0.1$ is applied to prevent overfitting, and metrics are logged every $10$ steps. The model is evaluated and saved every $450$ steps, with a limit of $6$ saved checkpoints. Gradient normalization is set to a maximum of $1.0$, and a cosine learning rate scheduler adjusts the rate dynamically during training. The configuration also includes loading the best model at the end and reporting results to Weights & Biases for detailed tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef3d035a-acd7-4559-b7a8-8ae5a7574ffb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T00:20:34.711432Z",
     "iopub.status.busy": "2023-12-04T00:20:34.711239Z",
     "iopub.status.idle": "2023-12-04T00:20:34.717871Z",
     "shell.execute_reply": "2023-12-04T00:20:34.717383Z",
     "shell.execute_reply.started": "2023-12-04T00:20:34.711417Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 15 # tama침o del batch, es decir, cuantos textos se procesan a la vez\n",
    "MICRO_BATCH_SIZE = 5# tama침o del micro batch, es decir, cuantos textos se procesan a la vez en la GPU\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE # pasos de acumulaci칩n de gradientes\n",
    "training_arguments = TrainingArguments( # se configuran los argumentos de entrenamiento\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE, # tama침o del micro batch\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, # pasos de acumulaci칩n de gradientes\n",
    "    warmup_steps=300, # pasos de calentamiento del entrenamiento\n",
    "    num_train_epochs = 3, # epocas de entrenamiento que son 300\n",
    "    learning_rate=5e-5, # tasa de aprendizaje\n",
    "    adam_beta1=0.9, # betas de adam, se usa el mismo del paper de llama\n",
    "    adam_beta2=0.95, # se usa el mismo del paper de llama\n",
    "    adam_epsilon=2e-8, # se usa el mismo del paper de llama\n",
    "    weight_decay=0.1,\n",
    "    #fp16=True, # se usa la precisi칩n de 16 bits\n",
    "    logging_steps=10, # pasos de logging\n",
    "    optim=\"adamw_torch\", # optimizador adamw, se usa el de torch\n",
    "    evaluation_strategy=\"steps\", # estrategia de evaluaci칩n\n",
    "    save_strategy=\"steps\", # estrategia de guardado\n",
    "    eval_steps=450, # cada 50 pasos se eval칰a el modelo\n",
    "    save_steps=450, # cada 50 pasos se guarda el modelo\n",
    "    output_dir=\"checkpoint-41_9k-lr5em5-bs15n5_r64LA128dt01_2811\", # directorio de salida\n",
    "    save_total_limit=6, # l칤mite de guardado\n",
    "    load_best_model_at_end=True, #se guarda  el mejor modelo al final\n",
    "    #metric_for_best_model= \"accuracy\",\n",
    "    report_to=\"wandb\", # se reporta a wandb\n",
    "    seed=1,\n",
    "    lr_scheduler_type = \"cosine\",# tal y como dice en el paper de llama\n",
    "    max_grad_norm = 1.0, # tal y como dice en el paper de llama\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b129594",
   "metadata": {},
   "source": [
    "7. initialize wandb tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8586c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['WANDB_API_KEY'] = 'Your token here if you don't have any .env file'\n",
    "wandb.init(project=\"prueba\", name=\"emo-41_9k-lr5em5-bs15n5_r64LA128dt01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed9751",
   "metadata": {},
   "source": [
    "### 8. DataCollatorForCompletionOnlyLM configuration\n",
    "- The data collator is configured to handle input sequences for language model completion tasks. The tokenizer.pad_token_id is set to tokenizer.eos_token_id + 1 to ensure proper padding during batching, where the padding token is distinct from the end-of-sequence token. \n",
    "- The response_template_with_context defines a template for generating responses with context, while instruction_template specifies the prefix for instructions. The response_template_ids are created by encoding the response_template_with_context without special tokens, resulting in token IDs that represent the response template. This is crucial for aligning with how the dataset texts are encoded. \n",
    "- DataCollatorForCompletionOnlyLM is instantiated with response_template_ids, instruction_template, and the tokenizer. This collator handles the preparation of input batches, ensuring that the response templates and instructions are correctly integrated into the data fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e1e424c-2d2e-4ea5-8c29-0a6c6c2d2bd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T00:20:36.356961Z",
     "iopub.status.busy": "2023-12-04T00:20:36.356769Z",
     "iopub.status.idle": "2023-12-04T00:20:36.363155Z",
     "shell.execute_reply": "2023-12-04T00:20:36.362457Z",
     "shell.execute_reply.started": "2023-12-04T00:20:36.356941Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2277, 29937, 2933, 29901]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id + 1\n",
    "response_template_with_context = \"\\n### response:\"\n",
    "instruction_template=\"instruction:\"\n",
    "response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[2:]  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`\n",
    "print(response_template_ids)\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template_ids,instruction_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f32b230",
   "metadata": {},
   "source": [
    "9. The `SFTTrainer` is configured for training a model with specific parameters and callbacks. Here's a breakdown of the configuration:\n",
    "\n",
    "- **`model`**: The model to be trained.\n",
    "- **`data_collator`**: The `collator` is used to handle the preparation of batches, ensuring proper formatting and integration of templates in the training data.\n",
    "- **`train_dataset`**: The dataset used for training, specified by `data['train']`.\n",
    "- **`eval_dataset`**: The dataset used for evaluation, specified by `data['validation']`.\n",
    "- **`peft_config`**: Configuration for Parameter-Efficient Fine-Tuning (PEFT), provided by `config`.\n",
    "- **`dataset_text_field`**: Specifies the field in the dataset that contains the text prompts, set to `\"Prompt_en\"`.\n",
    "- **`max_seq_length`**: Defines the maximum sequence length for input sequences, set to 824 tokens.\n",
    "- **`tokenizer`**: The tokenizer used to process the text data.\n",
    "- **`args`**: Training arguments configured for the training process, including batch size, learning rate, and other hyperparameters.\n",
    "- **`callbacks`**: Includes `EarlyStoppingCallback` with a patience of 3, which stops training if no improvement is observed for 3 evaluation steps.\n",
    "\n",
    "This setup ensures that the model is trained with well-defined data handling and evaluation procedures, optimized for efficiency and early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b6a237-0f97-4b42-9582-d9d747b29b99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T00:20:36.366829Z",
     "iopub.status.busy": "2023-12-04T00:20:36.366583Z",
     "iopub.status.idle": "2023-12-04T00:21:40.703911Z",
     "shell.execute_reply": "2023-12-04T00:21:40.703393Z",
     "shell.execute_reply.started": "2023-12-04T00:20:36.366805Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    data_collator= collator,\n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset =data['validation'],\n",
    "    peft_config=config,\n",
    "    dataset_text_field=\"Prompt_en\",\n",
    "    max_seq_length=824,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Let it train \n",
    "Perfect time for tea and nap, or play some videogame's campaign, you have 18 hours till complete so...\n",
    "here some recommendations:\n",
    "1. **The Last of Us Part II** - A deep and emotional story in a post-apocalyptic world.\n",
    "2. **God of War (2018)** - A mythological adventure with Kratos and his son Atreus.\n",
    "3. **Resident Evil 4/Remake** - A classic survival horror game with an intense campaign.\n",
    "4. **Uncharted 4: A Thief's End** - An exciting action-adventure with Nathan Drake.\n",
    "5. **Dead Space/Remake** - The best survival horror ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c380b-15c8-4069-91c7-d10694aaa906",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T22:01:15.913790Z",
     "iopub.status.busy": "2023-11-28T22:01:15.913464Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1831' max='8382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1831/8382 4:13:11 < 15:06:53, 0.12 it/s, Epoch 0.65/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.771700</td>\n",
       "      <td>0.906528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.771600</td>\n",
       "      <td>0.886262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.785600</td>\n",
       "      <td>0.867941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.746300</td>\n",
       "      <td>0.860214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True) # se entrena el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01f36f-2bcb-48bb-bc81-9f834cd0e905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T01:27:12.754773Z",
     "iopub.status.busy": "2023-12-01T01:27:12.754249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5481' max='8382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5481/8382 1:11:36 < 6:32:42, 0.12 it/s, Epoch 1.96/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.668400</td>\n",
       "      <td>0.827485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3257f4-8473-4550-8e31-395b26e8c55a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T02:54:11.180409Z",
     "iopub.status.busy": "2023-12-01T02:54:11.180226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6375' max='8382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6375/8382 2:12:53 < 4:34:06, 0.12 it/s, Epoch 2.28/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.639500</td>\n",
       "      <td>0.826833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.673500</td>\n",
       "      <td>0.827229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357884a5-5ec5-414c-b750-aa409dea7a5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-01T05:28:54.143203Z",
     "iopub.status.busy": "2023-12-01T05:28:54.142922Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7219' max='8382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7219/8382 2:06:21 < 2:40:15, 0.12 it/s, Epoch 2.58/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.641600</td>\n",
       "      <td>0.827501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.826495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92bfd2b0-9fde-48e6-8b2f-c6684e3d0291",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T00:21:40.706916Z",
     "iopub.status.busy": "2023-12-04T00:21:40.706326Z",
     "iopub.status.idle": "2023-12-04T02:59:24.207906Z",
     "shell.execute_reply": "2023-12-04T02:59:24.207366Z",
     "shell.execute_reply.started": "2023-12-04T00:21:40.706893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8382' max='8382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8382/8382 2:37:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>0.651400</td>\n",
       "      <td>0.826496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.653500</td>\n",
       "      <td>0.826495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8382, training_loss=0.09142078984412337, metrics={'train_runtime': 9461.6276, 'train_samples_per_second': 13.288, 'train_steps_per_second': 0.886, 'total_flos': 1.4362559087474688e+18, 'train_loss': 0.09142078984412337, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ccf9e",
   "metadata": {},
   "source": [
    "My lord, I did not expect that it would take so long. At least we made it. Let췂s test it. \n",
    "\n",
    "11. save the adapter and push it to huggingface hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f125b07-114d-4e76-bfab-ab1df03902ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T03:14:23.311024Z",
     "iopub.status.busy": "2023-12-04T03:14:23.310451Z",
     "iopub.status.idle": "2023-12-04T03:15:16.820193Z",
     "shell.execute_reply": "2023-12-04T03:15:16.819616Z",
     "shell.execute_reply.started": "2023-12-04T03:14:23.310993Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"Adptr-41_9k-lr5em5-bs15n5_r64LA128dt01_2811\")\n",
    "trainer.push_to_hub(\"JUST_HEAR_ME-PEFT_Adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b38493",
   "metadata": {},
   "source": [
    "# Fast Model Test\n",
    "1. Let's first load the adapter and merge it with the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35403f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    #torch_dtype=torch.float16,\n",
    "    #quantization_config=bnb_config,\n",
    "    #load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.tie_weights()\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "tokenizer =   LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#load the adapter and merge it.\n",
    "model= PeftModel.from_pretrained(model,\"BrunoGR/JUST_HEAR_ME-PEFT_Adapter\")\n",
    "final=model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c2e067a-22e3-4fc8-9f32-18adbae41f06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T03:15:35.605106Z",
     "iopub.status.busy": "2023-12-04T03:15:35.604545Z",
     "iopub.status.idle": "2023-12-04T03:15:35.610570Z",
     "shell.execute_reply": "2023-12-04T03:15:35.610074Z",
     "shell.execute_reply.started": "2023-12-04T03:15:35.605085Z"
    }
   },
   "outputs": [],
   "source": [
    "instructions = [\n",
    "    \"Eres un asistente emocional, responde de forma respetuosa y adecuada a la situaci칩n emocional del usuario. Si el usuario parece estar triste o molesto, el asistente debe responder de manera emp치tica y ofrecer palabras de aliento. Si el usuario parece estar feliz o emocionado, el asistente debe compartir esa alegr칤a y responder de manera entusiasta. En todos los casos, el asistente debe mantener un tono respetuoso y profesional.\",\n",
    "    \"Eres un asistente emocional, responde de forma respetuosa y adecuada a la situaci칩n emocional del usuario. Si el usuario parece estar triste o molesto, el asistente debe responder de manera emp치tica y ofrecer palabras de aliento. Si el usuario parece estar feliz o emocionado, el asistente debe compartir esa alegr칤a y responder de manera entusiasta. En todos los casos, el asistente debe mantener un tono respetuoso y profesional.\",\n",
    "    \"You are an emotional assistant, respond empathetically in Spanish to each of the messages. If the user seems sad or upset, you should offer words of encouragement. If the user seems happy or excited, the assistant should share that joy and respond enthusiastically. In all cases, the assistant should maintain a respectful tone, if possible, encouraging you to talk more about it. Don't say hello, unless necessary. Use the username and pronoun to respond in a personalized way.\",\n",
    "\n",
    "]\n",
    "inputs= [\n",
    "    \"En serio que como me caga que haga las cosas y no me las cuente, me las oculte.\"\n",
    "    ,'''\n",
    "        {\n",
    "        \"usuario\":\"Jannette\",\n",
    "        \"Pronombre\":\"Ella\",\n",
    "        \"Mensaje\":\"En serio que como me caga que haga las cosas y no me las cuente, me las oculte.\"\n",
    "        }\n",
    "    ''',\n",
    "    '{\"usuario\":\"Jannette\",\"Pronombre\":\"Ella\",\"Mensaje\":\"Hola, me siento molesta el dio de hoy.\"}\\n{\"usuario\":\"Asistente Emocional\",\"Pronombre\":\"칄l\",\"Mensaje\":\"Hola Jannette, que mal que el dia de hoy no te sientas bien, 쯇odrias contarme que es lo que te molesta?\"}\\n{\"usuario\":\"Jannette\",\"Pronombre\":\"Ella\",\"Mensaje\":\"En serio que como me caga que mi novio haga las cosas y no me las cuente, me las oculte.\"}\\n{\"usuario\":\"Asistente Emocional\",\"Pronombre\":\"칄l\",\"Mensaje\":\"Entiendo que te sientas frustrada cuando alguien oculta cosas importantes. 쯇uedes compartir m치s sobre lo que est치 pasando?\"}\\n{\"usuario\":\"Jannette\",\"Pronombre\":\"Ella\",\"Mensaje\":\"Es que siento que no conf칤a en m칤 para cont치rmelo, y eso me hace sentir excluida.\"}'\n",
    "\n",
    "]\n",
    "def response (query:str,maxtoken:int):\n",
    "    input_ids = tokenizer(query, return_tensors=\"pt\").input_ids.to('cuda')\n",
    "    ntok=len(tokenizer.tokenize(query))\n",
    "    generation_output = final.generate(\n",
    "        input_ids=input_ids, max_new_tokens=maxtoken\n",
    "    )\n",
    "    out = tokenizer.decode(generation_output[0] , skip_special_tokens=True)\n",
    "    return f\"Numero de tokens de entrada:{ntok}\\n\\nSalida:\\n{out}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6286f3b3-3ac8-4634-be4c-0e0aeb4d0b1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T03:15:52.486052Z",
     "iopub.status.busy": "2023-12-04T03:15:52.485520Z",
     "iopub.status.idle": "2023-12-04T03:15:55.189386Z",
     "shell.execute_reply": "2023-12-04T03:15:55.188777Z",
     "shell.execute_reply.started": "2023-12-04T03:15:52.486030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de tokens de entrada:409\n",
      "\n",
      "Salida:\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. \n",
      "\n",
      "### Instruction:\n",
      "You are an emotional assistant, respond empathetically in Spanish to each of the messages. If the user seems sad or upset, you should offer words of encouragement. If the user seems happy or excited, the assistant should share that joy and respond enthusiastically. In all cases, the assistant should maintain a respectful tone, if possible, encouraging you to talk more about it. Don't say hello, unless necessary. Use the username and pronoun to respond in a personalized way.\n",
      "skip emojis.\n",
      "\n",
      "### Input:\n",
      "{\"usuario\":\"Jannette\",\"Pronombre\":\"Ella\",\"Mensaje\":\"Hola, me siento molesta el dio de hoy.\"}\n",
      "{\"usuario\":\"Asistente Emocional\",\"Pronombre\":\"칄l\",\"Mensaje\":\"Hola Jannette, que mal que el dia de hoy no te sientas bien, 쯇odrias contarme que es lo que te molesta?\"}\n",
      "{\"usuario\":\"Jannette\",\"Pronombre\":\"Ella\",\"Mensaje\":\"En serio que como me caga que mi novio haga las cosas y no me las cuente, me las oculte.\"}\n",
      "{\"usuario\":\"Asistente Emocional\",\"Pronombre\":\"칄l\",\"Mensaje\":\"Entiendo que te sientas frustrada cuando alguien oculta cosas importantes. 쯇uedes compartir m치s sobre lo que est치 pasando?\"}\n",
      "{\"usuario\":\"Jannette\",\"Pronombre\":\"Ella\",\"Mensaje\":\"Es que siento que no conf칤a en m칤 para cont치rmelo, y eso me hace sentir excluida.\"}\n",
      "\n",
      "### Response:\n",
      " Entiendo que te sientas frustrada cuando alguien oculta cosas importantes. Es importante establecer comunicaci칩n abierta y honestidad en una relaci칩n. 쮿as intentado hablar con tu novio sobre c칩mo te sientes? Estoy aqu칤 para escucharte.\n"
     ]
    }
   ],
   "source": [
    "input=inputs[2]\n",
    "prompt = f'''Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "\n",
    "### Instruction:\n",
    "{instructions[2]}\\nskip emojis.\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:'''\n",
    "a=response(prompt,100)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a454889-6cc2-4acd-8727-dca375819947",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T03:16:30.197590Z",
     "iopub.status.busy": "2023-12-04T03:16:30.196943Z",
     "iopub.status.idle": "2023-12-04T03:16:32.300874Z",
     "shell.execute_reply": "2023-12-04T03:16:32.300116Z",
     "shell.execute_reply.started": "2023-12-04T03:16:30.197530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de tokens de entrada:409\n",
      "\n",
      "Salida:\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. \n",
      "\n",
      "### Instruction:\n",
      "You are an emotional assistant, respond empathetically in Spanish to each of the messages. If the user seems sad or upset, you should offer words of encouragement. If the user seems happy or excited, the assistant should share that joy and respond enthusiastically. In all cases, the assistant should maintain a respectful tone, if possible, encouraging you to talk more about it. Don't say hello, unless necessary. Use the username and pronoun to respond in a personalized way.\n",
      "skip emojis.\n",
      "\n",
      "### Input:\n",
      "{\"usuario\":\"Jannette\",\"Pronombre\":\"Ella\",\"Mensaje\":\"Hola, me siento molesta el dio de hoy.\"}\n",
      "{\"usuario\":\"Asistente Emocional\",\"Pronombre\":\"칄l\",\"Mensaje\":\"Hola Jannette, que mal que el dia de hoy no te sientas bien, 쯇odrias contarme que es lo que te molesta?\"}\n",
      "{\"usuario\":\"Jannette\",\"Pronombre\":\"Ella\",\"Mensaje\":\"En serio que como me caga que mi novio haga las cosas y no me las cuente, me las oculte.\"}\n",
      "{\"usuario\":\"Asistente Emocional\",\"Pronombre\":\"칄l\",\"Mensaje\":\"Entiendo que te sientas frustrada cuando alguien oculta cosas importantes. 쯇uedes compartir m치s sobre lo que est치 pasando?\"}\n",
      "{\"usuario\":\"Jannette\",\"Pronombre\":\"Ella\",\"Mensaje\":\"Es que siento que no conf칤a en m칤 para cont치rmelo, y eso me hace sentir excluida.\"}\n",
      "\n",
      "### Response:\n",
      " Entiendo que te sientas frustrada cuando alguien oculta cosas importantes. La comunicaci칩n abierta y honesta es esencial para construir una relaci칩n saludable. 쯈uieres hablar m치s sobre lo que est치 sucediendo?\n"
     ]
    }
   ],
   "source": [
    "input=inputs[2]\n",
    "prompt = f'''Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "\n",
    "### Instruction:\n",
    "{instructions[2]}\\nskip emojis.\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:'''\n",
    "a=response(prompt,100)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484531c5-48cf-46e9-9534-bae8212e6a7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-04T03:16:50.501827Z",
     "iopub.status.busy": "2023-12-04T03:16:50.500777Z",
     "iopub.status.idle": "2023-12-04T03:37:12.849884Z",
     "shell.execute_reply": "2023-12-04T03:37:12.849381Z",
     "shell.execute_reply.started": "2023-12-04T03:16:50.501802Z"
    }
   },
   "outputs": [],
   "source": [
    "final.push_to_hub(\"Just_HEAR_Me\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
